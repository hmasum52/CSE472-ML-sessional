{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn\n",
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom metrics functions\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    return np.sum(y_true * y_pred) / np.sum(y_pred)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    return np.sum(y_true * y_pred) / np.sum(y_true)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    return np.sum((1 - y_true) * (1 - y_pred)) / np.sum(1 - y_true)\n",
    "\n",
    "def false_discovery_rate(y_true, y_pred):\n",
    "    return 1 - precision(y_true, y_pred)\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test):\n",
    "    sc = StandardScaler()\n",
    "    X_train_scaled = sc.fit_transform(X_train)\n",
    "    X_test_scaled = sc.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom logistic regression class\n",
    "class MyLogisticRegression:\n",
    "\n",
    "    def __init__(self, n_features, \n",
    "                        lr=0.05, \n",
    "                        n_iters=2000, \n",
    "                        threshold=0,\n",
    "                        show_loss=False):\n",
    "        self.n_features = n_features\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = np.random.randn(n_features+1)\n",
    "        # Early terminate Gradient Descent if error in the training set becomes less than threshold\n",
    "        self.threshold = threshold \n",
    "        self.show_loss = show_loss\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / counts.sum()\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def _information_gain(self, X, y, feature):\n",
    "        original_entropy = self._entropy(y)\n",
    "\n",
    "        # Get the values and counts for the feature\n",
    "        values, counts = np.unique(X[:, feature], return_counts=True)\n",
    "\n",
    "        # Calculate the remainder\n",
    "        remainder = 0\n",
    "        for value, count in zip(values, counts):\n",
    "            remainder += count / counts.sum() * self._entropy(y[X[:, feature] == value])\n",
    "\n",
    "        # Calculate the information gain\n",
    "        info_gain = original_entropy - remainder\n",
    "        return info_gain\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _cost(self, X, y):\n",
    "        epsilon = 1e-10\n",
    "        y_pred = self._sigmoid(X @ self.weights)\n",
    "        cost = -(np.dot(y,np.log(y_pred+epsilon)) + np.dot((1 - y) , np.log(1 - y_pred+epsilon)) ) / len(y)\n",
    "        return cost\n",
    "\n",
    "    def _gradient(self, X, y):\n",
    "        y_pred = self._sigmoid(X @ self.weights)\n",
    "        # len(y)` represents the number of samples in the dataset. \n",
    "        # By dividing the sum of the gradients by the number of samples, we get the average gradient. \n",
    "        # This is done to ensure that the magnitude of the gradient is independent of the size of the dataset. \n",
    "        gradient = (X.T @ (y_pred - y)) / len(y)\n",
    "        return gradient\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # check shape\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"shape of X and y do not match\")\n",
    "\n",
    "        # check shape len\n",
    "        if len(X.shape) != 2:\n",
    "            raise ValueError(\"X must be 2 dimensional\")\n",
    "\n",
    "        # print(X.shape, y.shape)\n",
    "\n",
    "        # Calculate information gain for each feature\n",
    "        info_gains = [self._information_gain(X, y, feature) for feature in range(X.shape[1])]\n",
    "\n",
    "        # Get the indices of the features sorted by information gain\n",
    "        indices = np.argsort(info_gains)[::-1]\n",
    "\n",
    "        # Select the top n features\n",
    "        self.selected_features = indices[:self.n_features]\n",
    "\n",
    "        # freate a new array with only the selected features\n",
    "        X = X[:, self.selected_features]\n",
    "\n",
    "        # print(X.shape, y.shape)\n",
    "\n",
    "        # Add column for bias\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "\n",
    "        # apply gradient descent\n",
    "        for i in range(self.n_iters):\n",
    "            self.weights -= self.lr * self._gradient(X, y)\n",
    "            loss = self._cost(X, y)\n",
    "            if self.show_loss:\n",
    "                print(f\"epoch {i+1}, loss: {loss}\")\n",
    "            # early terminate if mse is less than threshold\n",
    "            if loss < self.threshold:\n",
    "                break\n",
    "        \n",
    "        # print(self._cost(X,y))\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        if self.selected_features is None:\n",
    "            raise Exception(\"model must be trained before prediction\")\n",
    "\n",
    "        # select the features\n",
    "        X = X[:, self.selected_features]\n",
    "        \n",
    "        # Add column for bias\n",
    "        X = np.concatenate([X, np.ones((X.shape[0], 1))], axis=1)\n",
    "\n",
    "        # predict\n",
    "        y_pred = self._sigmoid(X @ self.weights)\n",
    "\n",
    "        # convert probabilities to 0 or 1\n",
    "        y_pred = np.round(y_pred).astype(int)\n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y_true, y_pred):\n",
    "    print(f\"accuracy: {accuracy(y_true, y_pred):.4f}\")\n",
    "    print(f\"recall: {recall(y_true, y_pred):.4f}\")\n",
    "    print(f\"specificity: {specificity(y_true, y_pred):.4f}\")\n",
    "    print(f\"precision: {precision(y_true, y_pred):.4f}\")\n",
    "    print(f\"fdr: {false_discovery_rate(y_true, y_pred):.4f}\")\n",
    "    print(f\"f1: {f1(y_true, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, num_classifiers, n_features=10, threshold=0):\n",
    "        self.num_classifiers = num_classifiers\n",
    "        self.n_features = n_features\n",
    "        self.threshold = threshold\n",
    "        self.alphas = None\n",
    "        self.classifiers = None\n",
    "\n",
    "    def resample(self, X, y, weights):\n",
    "        indices = np.random.choice(len(X), len(X), p=weights)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "        alphas = []\n",
    "        classifiers = []\n",
    "\n",
    "        for k in range(self.num_classifiers):\n",
    "            X_resampled, y_resampled = self.resample(X, y, weights)\n",
    "            classifier = MyLogisticRegression(\n",
    "                n_features=self.n_features, n_iters=1000, threshold=self.threshold)\n",
    "            classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "            predictions = classifier.predict(X)\n",
    "            error = np.sum(weights * (predictions != y))\n",
    "\n",
    "            if error > 0.5:\n",
    "                continue\n",
    "\n",
    "            for i in range(n_samples):\n",
    "                if predictions[i] == y[i]:\n",
    "                    weights[i] *= error/(1-error)\n",
    "\n",
    "            alpha = np.log((1 - error) / error)\n",
    "           # weights = weights * np.exp(-alpha * y * predictions)\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "            alphas.append(alpha)\n",
    "            classifiers.append(classifier)\n",
    "\n",
    "        self.alphas = np.array(alphas)\n",
    "        self.classifiers = classifiers\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "\n",
    "        # normalize alpha\n",
    "        self.alphas = self.alphas/np.sum(self.alphas)\n",
    "\n",
    "        for alpha, classifier in zip(self.alphas, self.classifiers):\n",
    "            predictions += alpha * classifier.predict(X)\n",
    "\n",
    "    \n",
    "        predictions = (predictions >= 0.5).astype(int)\n",
    "\n",
    "        # print(f\"hi {len(predictions[predictions < 0])}\")\n",
    "        # print(predictions[predictions==0])\n",
    "        # print(predictions[predictions==1])\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def weighted_majority(self, X):\n",
    "        return self.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_telco_customer_churn_dataset():\n",
    "    # dataset 1: https://www.kaggle.com/datasets/blastchar/telco-customer-churn/\n",
    "    csv_path = \"datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # drop customerID\n",
    "    df.drop('customerID', axis=1, inplace=True)\n",
    "\n",
    "    columns = df.columns\n",
    "    # drop customer id, tenure, monthly charges, total charges\n",
    "    columns = columns.drop(['tenure', 'MonthlyCharges', 'TotalCharges'])\n",
    "\n",
    "    # preprocess data\n",
    "    for column in columns:\n",
    "        if df[column].dtype == 'object' and column != 'Churn':\n",
    "            dummies = pd.get_dummies(df[column], prefix=column)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "    # convert churn to 0 or 1\n",
    "    df['Churn'] = df['Churn'].astype('category').cat.codes\n",
    "\n",
    "    # convert total charges to float\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "    # drop rows with missing values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # split the data into 80% training and 20% testing using sklearn\n",
    "    # churn is the target\n",
    "    X = df.drop(['Churn'], axis=1).values\n",
    "    y = df['Churn'].values\n",
    "\n",
    "    # split the data into 80% training and 20% testing using sklearn\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    X_train, X_test = scale_data(X_train, X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_creditcard_dataset():\n",
    "  # read credit dataset from datasets/creditcard.csv\n",
    "  all_data_df = pd.read_csv('datasets/creditcard.csv')\n",
    "  # all_data_df.info()\n",
    "\n",
    "  # take all the rows with class 1\n",
    "  fraud_df = all_data_df[all_data_df['Class'] == 1]\n",
    "\n",
    "  # take 20000 rows with class 0\n",
    "  non_fraud_df = all_data_df[all_data_df['Class'] == 0].sample(20000)\n",
    "\n",
    "  df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "  X = df.drop(['Class'], axis=1).values\n",
    "  y = df['Class'].values\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "  # scale the data\n",
    "  X_train, X_test = scale_data(X_train, X_test)\n",
    "\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_adult_dataset():\n",
    "    # import adult dataset from datasets/adult folder\n",
    "    # https://archive.ics.uci.edu/ml/datasets/adult\n",
    "\n",
    "    # preprocess the data\n",
    "    # convert categorical data to numerical data\n",
    "    # split the data into 80% training and 20% testing using sklearn\n",
    "    columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', \n",
    "    'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "    train_df = pd.read_csv('datasets/adult/adult.data', names=columns)\n",
    "    train_df['train'] = 1\n",
    "\n",
    "    test_df = pd.read_csv('datasets/adult/adult.test', names=columns, skiprows=1)\n",
    "    test_df['train'] = 0\n",
    "\n",
    "    # concatenate train and test data\n",
    "    df = pd.concat([train_df, test_df])\n",
    "\n",
    "    # replace icome with 0 or 1\n",
    "    df['income'] = df['income'].str.replace('.', '')\n",
    "    df['income'].unique()\n",
    "\n",
    "    # education and education-num are the same\n",
    "    # drop education\n",
    "    df.drop('education', axis=1, inplace=True)\n",
    "\n",
    "    # \n",
    "    df['income'] = df['income'].astype('category').cat.codes\n",
    "\n",
    "    # convert categorical data to numerical data\n",
    "    for column in columns:\n",
    "        if column in [\n",
    "            'workclass', 'education-num', \n",
    "            'marital-status', 'occupation', 'relationship',\n",
    "            'race', 'sex', 'native-country'  ]:\n",
    "            dummies = pd.get_dummies(df[column], prefix=column)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "    # train test split\n",
    "    train_df = df[df['train'] == 1]\n",
    "    test_df = df[df['train'] == 0]\n",
    "\n",
    "    X_train = train_df.drop(['income', 'train'], axis=1).values\n",
    "    y_train = train_df['income'].values\n",
    "\n",
    "\n",
    "    X_test = test_df.drop(['income', 'train'], axis=1).values\n",
    "    y_test = test_df['income'].values\n",
    "\n",
    "    # scale data\n",
    "    X_train, X_test = scale_data(X_train, X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X_train, X_test, y_train, y_test, n, n_adaboost=20):\n",
    "  # create the model\n",
    "  # model = MyLogisticRegression(n_features=X_train.shape[1])\n",
    "  n = int(X_train.shape[1] * 0.8)\n",
    "  model = MyLogisticRegression(n_features=n, show_loss=False, threshold=0)\n",
    "\n",
    "  # train the model\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # predict train data\n",
    "  y_pred = model.predict(X_train)\n",
    "  print(\"train data\")\n",
    "  report(y_train, y_pred)\n",
    "\n",
    "  print()\n",
    "\n",
    "  # predict test data\n",
    "  y_pred = model.predict(X_test)\n",
    "  print(\"test data\")\n",
    "  report(y_test, y_pred)\n",
    "\n",
    "  print()\n",
    "\n",
    "  K = [5, 10, 15, 20]\n",
    "  for k in K:\n",
    "      print(f\"num_classifiers: {k}\")\n",
    "      model = AdaBoost(num_classifiers=k, n_features=n_adaboost, threshold=0.5)\n",
    "      model.fit(X_train, y_train)\n",
    "\n",
    "      # predict train data\n",
    "      y_pred = model.predict(X_train)\n",
    "      print(f\"train: k={k}, accuracy: {accuracy(y_train, y_pred):.4f}\")\n",
    "\n",
    "      # predict test data\n",
    "      y_pred = model.predict(X_test)\n",
    "      print(f\"test: k={k}, accuracy: {accuracy(y_test, y_pred):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5625, 45)\n",
      "(1407, 45)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_telco_customer_churn_dataset()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "accuracy: 0.8039\n",
      "recall: 0.5473\n",
      "specificity: 0.8972\n",
      "precision: 0.6594\n",
      "fdr: 0.3406\n",
      "f1: 0.5982\n",
      "\n",
      "test data\n",
      "accuracy: 0.8003\n",
      "recall: 0.5312\n",
      "specificity: 0.8960\n",
      "precision: 0.6447\n",
      "fdr: 0.3553\n",
      "f1: 0.5825\n",
      "\n",
      "num_classifiers: 5\n",
      "train: k=5, accuracy: 0.7845\n",
      "test: k=5, accuracy: 0.7740\n",
      "\n",
      "num_classifiers: 10\n",
      "train: k=10, accuracy: 0.7705\n",
      "test: k=10, accuracy: 0.7676\n",
      "\n",
      "num_classifiers: 15\n",
      "train: k=15, accuracy: 0.7714\n",
      "test: k=15, accuracy: 0.7690\n",
      "\n",
      "num_classifiers: 20\n",
      "train: k=20, accuracy: 0.7641\n",
      "test: k=20, accuracy: 0.7662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = int(X_train.shape[1] * 0.8)\n",
    "run_model(X_train, X_test, y_train, y_test, n, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "accuracy: 0.9953\n",
      "recall: 0.8138\n",
      "specificity: 0.9996\n",
      "precision: 0.9776\n",
      "fdr: 0.0224\n",
      "f1: 0.8882\n",
      "\n",
      "test data\n",
      "accuracy: 0.9939\n",
      "recall: 0.8190\n",
      "specificity: 0.9990\n",
      "precision: 0.9596\n",
      "fdr: 0.0404\n",
      "f1: 0.8837\n",
      "\n",
      "num_classifiers: 5\n",
      "train: k=5, accuracy: 0.9706\n",
      "test: k=5, accuracy: 0.9656\n",
      "\n",
      "num_classifiers: 10\n",
      "train: k=10, accuracy: 0.9900\n",
      "test: k=10, accuracy: 0.9846\n",
      "\n",
      "num_classifiers: 15\n",
      "train: k=15, accuracy: 0.9894\n",
      "test: k=15, accuracy: 0.9856\n",
      "\n",
      "num_classifiers: 20\n",
      "train: k=20, accuracy: 0.9881\n",
      "test: k=20, accuracy: 0.9859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_creditcard_dataset()\n",
    "n = int(X_train.shape[1] * 0.8)\n",
    "run_model(X_train, X_test, y_train, y_test, n, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data\n",
      "accuracy: 0.8483\n",
      "recall: 0.6083\n",
      "specificity: 0.9244\n",
      "precision: 0.7184\n",
      "fdr: 0.2816\n",
      "f1: 0.6588\n",
      "\n",
      "test data\n",
      "accuracy: 0.8471\n",
      "recall: 0.6001\n",
      "specificity: 0.9235\n",
      "precision: 0.7082\n",
      "fdr: 0.2918\n",
      "f1: 0.6497\n",
      "\n",
      "num_classifiers: 5\n",
      "train: k=5, accuracy: 0.8232\n",
      "test: k=5, accuracy: 0.8235\n",
      "\n",
      "num_classifiers: 10\n",
      "train: k=10, accuracy: 0.8266\n",
      "test: k=10, accuracy: 0.8284\n",
      "\n",
      "num_classifiers: 15\n",
      "train: k=15, accuracy: 0.8279\n",
      "test: k=15, accuracy: 0.8272\n",
      "\n",
      "num_classifiers: 20\n",
      "train: k=20, accuracy: 0.8272\n",
      "test: k=20, accuracy: 0.8256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_adult_dataset()\n",
    "n = 20\n",
    "run_model(X_train, X_test, y_train, y_test, n, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
